{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b225c2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95f22218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bd0aca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1f2572c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:06<00:00, 1.46MB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 46.2kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:02<00:00, 707kB/s] \n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 1.35MB/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = MNIST(root='data', train=True, transform=transform, download=True)\n",
    "test_data = MNIST(root='data', train=False, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b287a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c0321db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(16 * 7 * 7, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99872b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNN().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55445bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3] - Avg Loss: 0.3406\n",
      "Epoch [2/3] - Avg Loss: 0.0901\n",
      "Epoch [3/3] - Avg Loss: 0.0621\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        # images = images.view(images.size(0), -1).to(device)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() \n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] - Avg Loss: {avg_loss:.4f}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0034aa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a908fe41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 98.02 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        # images = images.view(images.size(0), -1).to(device)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        predictions = outputs.argmax(dim=1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy of the model on the test images: {100 * correct / total} %')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3525dca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to digit_classifier_cnn.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'digit_classifier_cnn.pth')\n",
    "print('Model saved to digit_classifier_cnn.pth')\n",
    "# import os\n",
    "# print(os.getcwd())\n",
    "# from google.colab import files\n",
    "# files.download(\"digit_classifier_cnn.pth\")\n",
    "# print('Model downloaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fdf3c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 0\n",
      "True Label: 7\n",
      "Predicted Label: 7\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "image, true_label = test_data[index]\n",
    "\n",
    "plt.imshow(image.squeeze(), cmap='gray')\n",
    "plt.title(f'True Label: {true_label}')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "image = image.unsqueeze(0).to(device)  # shape: (1, 1, 28, 28)\n",
    "# image_flat = image.view(1, -1).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(image)\n",
    "    predicted_label = output.argmax(dim=1).item()\n",
    "\n",
    "\n",
    "print(f'Index: {index}')\n",
    "print(f'True Label: {true_label}')\n",
    "print(f'Predicted Label: {predicted_label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c69bf23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Predictions: 981 out of 1000\n",
      "Incorrect Predictions: 19 out of 1000\n",
      "Accuracy over 1000 random samples: 98.1 %\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "n = 1000\n",
    "cnt = 0\n",
    "for _ in range(n):\n",
    "    index = random.randint(0, len(test_data)-1)\n",
    "    image, true_label = test_data[index]\n",
    "\n",
    "    # plt.imshow(image.squeeze(), cmap='gray')\n",
    "    # plt.title(f'True Label: {true_label}')\n",
    "    # plt.axis('off')\n",
    "    # plt.show()\n",
    "\n",
    "    image = image.unsqueeze(0).to(device)  # shape: (1, 1, 28, 28)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        predicted_label = output.argmax(dim=1).item()\n",
    "\n",
    "    if true_label == predicted_label:\n",
    "        cnt += 1\n",
    "\n",
    "    # print('---')\n",
    "    # print(f'Index: {index}')\n",
    "    # print(f'True Label: {true_label}')\n",
    "    # print(f'Predicted Label: {predicted_label}')\n",
    "print(f'Correct Predictions: {cnt} out of {n}')\n",
    "print(f'Incorrect Predictions: {n - cnt} out of {n}')\n",
    "print(f'Accuracy over {n} random samples: {100 * cnt / n} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
